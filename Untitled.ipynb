{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\toutput\toutput\t名詞-固有名詞-組織\t\t\n",
      ":\t:\t:\t名詞-サ変接続\t\t\n",
      "F\tF\tF\t名詞-一般\t\t\n",
      "050\t050\t050\t名詞-数\t\t\n",
      "さん\tサン\tさん\t名詞-接尾-人名\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "今日\tキョウ\t今日\t名詞-副詞可能\t\t\n",
      "何\tナン\t何\t名詞-数\t\t\n",
      "時間\tジカン\t時間\t名詞-接尾-助数詞\t\t\n",
      "あっ\tアッ\tある\t動詞-自立\t五段・ラ行\t連用タ接続\n",
      "た\tタ\tた\t助動詞\t特殊・タ\t基本形\n",
      "の\tノ\tの\t助詞-終助詞\t\t\n",
      "？\t？\t？\t記号-一般\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger (\"-Ochasen\")\n",
    "text = mecab.parse (\"output: F050さんは、今日何時間あったの？\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mecab 大文字小文字に注意\n",
    "import MeCab\n",
    "# datetime\n",
    "import time\n",
    "# 引数取得\n",
    "import sys\n",
    "from sys import argv\n",
    "\n",
    "# 引数の取得\n",
    "input_file_name= \"make-meidai-dialogue-master/sequence.txt\"\n",
    "\n",
    "# 解析対象テキストファイルを開く\n",
    "with open('make-meidai-dialogue-master/sequence.txt', mode='rt', encoding='utf-8') as f:\n",
    "    read_data = list(f)\n",
    "for  i in range (len(read_data)):\n",
    "    if i % 2 != 0:\n",
    "        # ファイルを読み込む\n",
    "        data = read_data[i]\n",
    "        # 分かち書きのみ出力する設定にする\n",
    "        mecab = MeCab.Tagger(\"-Owakati\")\n",
    "        text = mecab.parse(data)\n",
    "        mecab.parse('')\n",
    "        #テキストファイルへの書き込み（上書き）\n",
    "        with open(\"output.txt\", \"a\") as f:  # w:上書きモード\n",
    "            f.write(\"\\n\"+text)                  # 書き込めるのは文字列型のみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  \n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.models.rnn import rnn_cell\n",
    "#from tensorflow.models.rnn import seq2seq\n",
    "\n",
    "\n",
    "\n",
    "class Seq2SeqModel(object):   \n",
    "\n",
    "  def __init__(self, source_vocab_size, target_vocab_size, buckets, size,\n",
    "               num_layers, max_gradient_norm, batch_size, learning_rate,\n",
    "               learning_rate_decay_factor, use_lstm=False,\n",
    "               num_samples=512, forward_only=False):\n",
    "\n",
    "    self.source_vocab_size = source_vocab_size    \n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.buckets = buckets\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "        self.learning_rate * learning_rate_decay_factor)\n",
    "    self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "\n",
    "    output_projection = None\n",
    "    softmax_loss_function = None\n",
    "\n",
    "    if num_samples > 0 and num_samples < self.target_vocab_size:     \n",
    "      with tf.device(\"/cpu:0\"):\n",
    "        w = tf.get_variable(\"proj_w\", [size, self.target_vocab_size])            \n",
    "        w_t = tf.transpose(w)\n",
    "        b = tf.get_variable(\"proj_b\", [self.target_vocab_size])\n",
    "      output_projection = (w, b)                                        \n",
    "\n",
    "\n",
    "      def sampled_loss(inputs, labels):\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "          labels = tf.reshape(labels, [-1, 1])\n",
    "          return tf.nn.sampled_softmax_loss(w_t, b, inputs, labels, num_samples,\n",
    "                                            self.target_vocab_size)\n",
    "      softmax_loss_function = sampled_loss\n",
    "\n",
    "\n",
    "    single_cell = rnn_cell.GRUCell(size)\n",
    "    if use_lstm:                                    \n",
    "      single_cell = rnn_cell.BasicLSTMCell(size)    \n",
    "    cell = single_cell\n",
    "    if num_layers > 1:\n",
    "      cell = rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "\n",
    "    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "      return seq2seq.embedding_attention_seq2seq(\n",
    "          encoder_inputs, decoder_inputs, cell, source_vocab_size,\n",
    "          target_vocab_size, output_projection=output_projection,\n",
    "          feed_previous=do_decode)\n",
    "\n",
    "    self.encoder_inputs = []\n",
    "    self.decoder_inputs = []\n",
    "    self.target_weights = []\n",
    "    for i in xrange(buckets[-1][0]):  \n",
    "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "    for i in xrange(buckets[-1][1] + 1):\n",
    "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "      self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "\n",
    "\n",
    "    targets = [self.decoder_inputs[i + 1]\n",
    "               for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\n",
    "\n",
    "    if forward_only:\n",
    "      self.outputs, self.losses = seq2seq.model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets, self.target_vocab_size,\n",
    "          lambda x, y: seq2seq_f(x, y, True),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "\n",
    "      if output_projection is not None:\n",
    "        for b in xrange(len(buckets)):\n",
    "          self.outputs[b] = [tf.matmul(output, output_projection[0]) +\n",
    "                             output_projection[1]\n",
    "                             for output in self.outputs[b]]\n",
    "    else:\n",
    "      self.outputs, self.losses = seq2seq.model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets, self.target_vocab_size,\n",
    "          lambda x, y: seq2seq_f(x, y, False),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    if not forward_only:\n",
    "      self.gradient_norms = []\n",
    "      self.updates = []\n",
    "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "      for b in xrange(len(buckets)):\n",
    "        gradients = tf.gradients(self.losses[b], params)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                         max_gradient_norm)\n",
    "        self.gradient_norms.append(norm)\n",
    "        self.updates.append(opt.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "    self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only):\n",
    "\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    if len(encoder_inputs) != encoder_size:\n",
    "      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "    if len(decoder_inputs) != decoder_size:\n",
    "      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "    if len(target_weights) != decoder_size:\n",
    "      raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "\n",
    "    input_feed = {}\n",
    "    for l in xrange(encoder_size):\n",
    "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "    for l in xrange(decoder_size):\n",
    "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "    last_target = self.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)   \n",
    "\n",
    "\n",
    "    if not forward_only:\n",
    "      output_feed = [self.updates[bucket_id], \n",
    "                     self.gradient_norms[bucket_id], \n",
    "                     self.losses[bucket_id]]  \n",
    "    else:\n",
    "      output_feed = [self.losses[bucket_id]]  \n",
    "      for l in xrange(decoder_size):  \n",
    "        output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "    outputs = session.run(output_feed, input_feed)\n",
    "    if not forward_only:\n",
    "      return outputs[1], outputs[2], None  \n",
    "    else:\n",
    "      return None, outputs[0], outputs[1:]  \n",
    "\n",
    "\n",
    "\n",
    "  def get_batch(self, data, bucket_id):\n",
    "\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]   \n",
    "    encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "    for _ in xrange(self.batch_size):                                     \n",
    "      encoder_input, decoder_input = random.choice(data[bucket_id])        \n",
    "\n",
    "      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input)) \n",
    "      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))       \n",
    "\n",
    "      decoder_pad_size = decoder_size - len(decoder_input) - 1               \n",
    "      decoder_inputs.append([data_utils.GO_ID] + decoder_input +             \n",
    "                            [data_utils.PAD_ID] * decoder_pad_size)          \n",
    "\n",
    "\n",
    "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "\n",
    "    for length_idx in xrange(encoder_size):                                  \n",
    "      batch_encoder_inputs.append(\n",
    "          np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "\n",
    "    for length_idx in xrange(decoder_size):\n",
    "      batch_decoder_inputs.append(\n",
    "          np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "\n",
    "      batch_weight = np.ones(self.batch_size, dtype=np.float32)           \n",
    "      for batch_idx in xrange(self.batch_size):\n",
    "\n",
    "        if length_idx < decoder_size - 1:\n",
    "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:   \n",
    "          batch_weight[batch_idx] = 0.0\n",
    "      batch_weights.append(batch_weight)\n",
    "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "from six.moves import urllib\n",
    "\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(r\"\\d\")\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "  words = []\n",
    "  for space_separated_fragment in sentence.strip().split():\n",
    "    words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n",
    "\n",
    "  return [w for w in words if w]\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "\n",
    "  if os.path.exists(vocabulary_path):       \n",
    "    print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, data_path))\n",
    "    vocab = {}\n",
    "\n",
    "    f = open(data_path,\"r\")\n",
    "    counter = 0\n",
    "    for line in f:\n",
    "      counter += 1\n",
    "      if counter % 100 == 0:\n",
    "        print(\"  processing line %d\" % counter)\n",
    "      tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "      for w in tokens:\n",
    "        word = re.sub(_DIGIT_RE, \"0\", w) if normalize_digits else w   \n",
    "        if word in vocab:       \n",
    "          vocab[word] += 1\n",
    "        else:\n",
    "          vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocabulary_size:\n",
    "      vocab_list = vocab_list[:max_vocabulary_size]       \n",
    "\n",
    "    vocab_file = open(vocabulary_path,\"w\")\n",
    "    for w in vocab_list:\n",
    "      vocab_file.write(w + \"\\n\")\n",
    "    vocab_file.close()  \n",
    "    f.close()\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "\n",
    "  if os.path.exists(vocabulary_path):       \n",
    "    rev_vocab = []\n",
    "\n",
    "    f = open(vocabulary_path,\"r\")\n",
    "    rev_vocab.extend(f.readlines())\n",
    "    rev_vocab = [line.strip() for line in rev_vocab]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    f.close()\n",
    "    return vocab, rev_vocab\n",
    "\n",
    "  else:\n",
    "    raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary,\n",
    "                          tokenizer=None, normalize_digits=True):\n",
    "\n",
    "  if tokenizer:\n",
    "    words = tokenizer(sentence)\n",
    "  else:\n",
    "    words = basic_tokenizer(sentence)\n",
    "  if not normalize_digits:\n",
    "    return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "  return [vocabulary.get(re.sub(_DIGIT_RE, \"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "\n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "\n",
    "  print(\"Tokenizing data in %s\" % data_path)\n",
    "  vocab, _ = initialize_vocabulary(vocabulary_path)                 \n",
    "  data_file = open(data_path,\"r\")\n",
    "  tokens_file = open(target_path,\"w\")\n",
    "  counter = 0\n",
    "  for line in data_file:\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "      print(\"  tokenizing line %d\" % counter)\n",
    "    token_ids = sentence_to_token_ids(line, vocab, tokenizer,         \n",
    "                                            normalize_digits)\n",
    "    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")   \n",
    "  data_file.close()\n",
    "  tokens_file.close()      \n",
    "\n",
    "def prepare_wmt_data(data_dir, in_vocabulary_size, out_vocabulary_size):\n",
    "\n",
    "  train_path = os.path.join(data_dir, \"train_data\")               \n",
    "  dev_path = os.path.join(data_dir, \"test_data\")                     \n",
    "\n",
    "\n",
    "  out_vocab_path = os.path.join(data_dir, \"vocab_out.txt\" )   \n",
    "  in_vocab_path = os.path.join(data_dir, \"vocab_in.txt\" )   \n",
    "  create_vocabulary(out_vocab_path, train_path + \"_out.txt\", out_vocabulary_size)    \n",
    "  create_vocabulary(in_vocab_path, train_path + \"_in.txt\", in_vocabulary_size)    \n",
    "\n",
    "  out_train_ids_path = train_path + (\"_ids_out.txt\" )         \n",
    "  in_train_ids_path = train_path + (\"_ids_in.txt\" )        \n",
    "  data_to_token_ids(train_path + \"_out.txt\", out_train_ids_path, out_vocab_path)    \n",
    "  data_to_token_ids(train_path + \"_in.txt\", in_train_ids_path, in_vocab_path)    \n",
    "\n",
    "\n",
    "  out_dev_ids_path = dev_path + (\"_ids_out.txt\" )             \n",
    "  in_dev_ids_path = dev_path + (\"_ids_in.txt\" )             \n",
    "  data_to_token_ids(dev_path + \"_out.txt\", out_dev_ids_path, out_vocab_path)         \n",
    "  data_to_token_ids(dev_path + \"_in.txt\", in_dev_ids_path, in_vocab_path)         \n",
    "\n",
    "  return (in_train_ids_path, out_train_ids_path,           \n",
    "          in_dev_ids_path, out_dev_ids_path,\n",
    "          in_vocab_path, out_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --learning_rate: conflicting option string: --learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fa7a0af7bfd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n\u001b[1;32m     31\u001b[0m                           \"Learning rate decays by this much.\")\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mDEFINE_float\u001b[0;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0m_define_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m _allowed_symbols = [\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m_define_helper\u001b[0;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                               type=flagtype)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1548\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yu-ka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1506\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --learning_rate: conflicting option string: --learning_rate"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n",
    "                          \"Learning rate decays by this much.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                          \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                            \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 1024, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_integer(\"in_vocab_size\", 50, \"input vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"out_vocab_size\", 50, \"output vocabulary size.\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"datas\", \"Data directory\")       \n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"datas\", \"Training directory.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
    "                            \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 100,\n",
    "                            \"How many training steps to do per checkpoint.\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False,\n",
    "                            \"Set to True for interactive decoding.\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
    "                            \"Run a self-test if this is set to True.\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  source_file = open(source_path,\"r\")\n",
    "  target_file = open(target_path,\"r\")\n",
    "\n",
    "  source, target = source_file.readline(), target_file.readline()     \n",
    "  counter = 0\n",
    "  while source and target and (not max_size or counter < max_size):\n",
    "    counter += 1\n",
    "    if counter % 50 == 0:                         \n",
    "      print(\"  reading data line %d\" % counter)\n",
    "      sys.stdout.flush()\n",
    "\n",
    "    source_ids = [int(x) for x in source.split()]   \n",
    "    target_ids = [int(x) for x in target.split()]    \n",
    "    target_ids.append(data_utils.EOS_ID)             \n",
    "    for bucket_id, (source_size, target_size) in enumerate(_buckets):        \n",
    "      if len(source_ids) < source_size and len(target_ids) < target_size:    \n",
    "        data_set[bucket_id].append([source_ids, target_ids])                 \n",
    "        break\n",
    "    source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "def create_model(session, forward_only):\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS.in_vocab_size, FLAGS.out_vocab_size, _buckets,                           \n",
    "      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,      \n",
    "      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,                       \n",
    "      forward_only=forward_only)                                                    \n",
    "\n",
    "  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)                             \n",
    "  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):                              \n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)          \n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)                        \n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())                                      \n",
    "  return model                                                                      \n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "  print(\"Preparing data in %s\" % FLAGS.data_dir)                                \n",
    "  in_train, out_train, in_dev, out_dev, _, _ = data_utils.prepare_wmt_data(           \n",
    "      FLAGS.data_dir, FLAGS.in_vocab_size, FLAGS.out_vocab_size)                     \n",
    "\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "\n",
    "\n",
    "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))       \n",
    "    model = create_model(sess, False)                                               \n",
    "\n",
    "    print (\"Reading development and training data (limit: %d).\"     \n",
    "           % FLAGS.max_train_data_size)                                             \n",
    "    dev_set = read_data(in_dev, out_dev)                                             \n",
    "    train_set = read_data(in_train, out_train, FLAGS.max_train_data_size)            \n",
    "\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]         \n",
    "    train_total_size = float(sum(train_bucket_sizes))                               \n",
    "\n",
    "\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size     \n",
    "                           for i in xrange(len(train_bucket_sizes))]              \n",
    "\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "\n",
    "      random_number_01 = np.random.random_sample()                     \n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))      \n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(   \n",
    "          train_set, bucket_id)                                           \n",
    "\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, \n",
    "                                   target_weights, bucket_id, False)      \n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "\n",
    "      if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "\n",
    "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "\n",
    "\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              dev_set, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    print (\"Hello!!\")\n",
    "    model = create_model(sess, True)                         \n",
    "    model.batch_size = 1  \n",
    "\n",
    "    in_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab_in.txt\")     \n",
    "    out_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab_out.txt\" )\n",
    "\n",
    "    in_vocab, _ = data_utils.initialize_vocabulary(in_vocab_path)        \n",
    "    _, rev_out_vocab = data_utils.initialize_vocabulary(out_vocab_path)    \n",
    "\n",
    "\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()    \n",
    "    while sentence:\n",
    "\n",
    "      token_ids = data_utils.sentence_to_token_ids(sentence, in_vocab)   \n",
    "\n",
    "      bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                       if _buckets[b][0] > len(token_ids)])               \n",
    "\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,      \n",
    "                                       target_weights, bucket_id, True)\n",
    "\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]       \n",
    "\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]                      \n",
    "\n",
    "      print(\" \".join([rev_out_vocab[output] for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()                                             \n",
    "\n",
    "\n",
    "def self_test():\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    print(\"Self-test for neural translation model.\")\n",
    "\n",
    "    model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "    for _ in xrange(5):  \n",
    "      bucket_id = random.choice([0, 1])\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          data_set, bucket_id)\n",
    "      model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
    "                 bucket_id, False)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if FLAGS.self_test:\n",
    "    self_test()\n",
    "  elif FLAGS.decode:\n",
    "    decode()\n",
    "  else:\n",
    "    train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output: F050さんは、今日何時間あったの？\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
